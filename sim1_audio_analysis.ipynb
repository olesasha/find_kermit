{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Feature Analyses SIM 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from scripts.load_data import check_and_load\n",
    "from scripts.extract_audio_features import extract_zcr, extract_loudness, extract_rhythm, create_target_variable\n",
    "from scripts.nested_cv import partition_feature_df, nested_cross_validation\n",
    "\n",
    "from scipy.signal import correlate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define paths\n",
    "# data_path = \"../ground_truth_data/trimmed_videos\"\n",
    "# frames_output_dir = \"../ground_truth_data/trimmed_videos/frames\"\n",
    "# audio_output_dir = \"../ground_truth_data/trimmed_videos/audio\"\n",
    "# annotations_path = \"../ground_truth_data/trimmed_videos\"\n",
    "\n",
    "data_path = \"../ground_truth_data\"\n",
    "frames_output_dir = \"../ground_truth_data/frames\"\n",
    "audio_output_dir = \"../ground_truth_data/audio\"\n",
    "annotations_path = \"../ground_truth_data\"\n",
    "\n",
    "muppet_files = {\n",
    "    \"Muppets-02-01-01.avi\": \"GroundTruth_Muppets-02-01-01.csv\",\n",
    "    \"Muppets-02-04-04.avi\": \"GroundTruth_Muppets-02-04-04.csv\",\n",
    "    \"Muppets-03-04-03.avi\": \"GroundTruth_Muppets-03-04-03.csv\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames and audio are already extracted.\n",
      "Loading audio segments...\n",
      "Loaded 3 audio files.\n",
      "Loaded audio segments for 3 videos.\n",
      "Loaded frames for 3 videos.\n",
      "Number of videos with frames: 3\n",
      "Video 0 has 38681 frames.\n",
      "Video 1 has 38706 frames.\n",
      "Video 2 has 38498 frames.\n"
     ]
    }
   ],
   "source": [
    "# TODO: add code to overwrite frames or audio in case only one exists\n",
    "annotations, audio_data, frames = check_and_load(data_path, frames_output_dir, audio_output_dir, annotations_path, muppet_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loudness\n",
    "\n",
    "    - Description: Measures overall signal strength through RMS of sample amplitudes.\n",
    "    - Use Case: Effective when differentiating characters based on the volume or energy of their speech or sound.\n",
    "\n",
    "Fundamental Frequency\n",
    "\n",
    "    - Description: Extracts the pitch of the audio signal using methods like zero-crossing rate (ZCR).\n",
    "    - Use Case: Useful for identifying characters with distinct pitch or tonal qualities in their voices (e.g., Kermit's high-pitched voice).\n",
    "\n",
    "Rhythm Detection\n",
    "\n",
    "    - Description: Uses autocorrelation to find repeating patterns across frames. Statistical moments indicate the presence of rhythm.\n",
    "    - Use Case: Ideal for characters with unique speech cadences or rhythmic patterns (e.g., the conversational style of Waldorf and Statler)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ZCR features\n",
    "zcr_features = extract_zcr(audio_data)\n",
    "\n",
    "# Extract loudness features\n",
    "loudness_features = extract_loudness(audio_data)\n",
    "\n",
    "# Extract rhythm features\n",
    "rhythm_features = extract_rhythm(audio_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(115885, 5)\n"
     ]
    }
   ],
   "source": [
    "extracted_features_df = []\n",
    "\n",
    "for video_idx, (audio_entry, zcr, loudness, rhythm) in enumerate(zip(audio_data, zcr_features, loudness_features, rhythm_features)):\n",
    "    if zcr is not None and loudness is not None and rhythm is not None:\n",
    "        num_frames = min(len(zcr), len(loudness), len(rhythm))\n",
    "        for frame_idx in range(num_frames):\n",
    "            extracted_features_df.append({\n",
    "                \"video_idx\": video_idx,\n",
    "                \"frame_idx\": frame_idx,\n",
    "                \"loudness_rms\": loudness[frame_idx],\n",
    "                \"zcr\": zcr[frame_idx],\n",
    "                \"rhythm\": rhythm[frame_idx]\n",
    "            })\n",
    "\n",
    "extracted_features_df = pd.DataFrame(extracted_features_df)\n",
    "print(extracted_features_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video 0: (38681, 5)\n",
      "Video 1: (38706, 5)\n",
      "Video 2: (38498, 5)\n"
     ]
    }
   ],
   "source": [
    "for i in extracted_features_df[\"video_idx\"].unique():\n",
    "    print(f\"Video {i}: {extracted_features_df[extracted_features_df['video_idx'] == i].shape}\")\n",
    "\n",
    "\n",
    "# Video 0 has 38681 frames.\n",
    "# Video 1 has 38706 frames.\n",
    "# Video 2 has 38498 frames.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from filenames to video indices\n",
    "video_idx_map = {filename: idx for idx, filename in enumerate(muppet_files.keys())}\n",
    "\n",
    "# Prepare ground truth data with corrected video_idx\n",
    "ground_truth_data = []\n",
    "for video_filename, annotation_df in annotations.items():\n",
    "    video_idx = video_idx_map[video_filename]  # Map video filename to its index\n",
    "    for _, row in annotation_df.iterrows():\n",
    "        ground_truth_data.append({\n",
    "            'video_idx': video_idx,  # Use mapped video index\n",
    "            'frame_idx': row['Frame_number'],  # Assuming Frame_number exists\n",
    "            'Kermit': row['Kermit'],  # Assuming Kermit is a column in the annotation\n",
    "            'Audio_StatlerWaldorf': row['Audio_StatlerWaldorf']  # Assuming this column exists\n",
    "        })\n",
    "\n",
    "# Create a DataFrame for ground truth\n",
    "ground_truth_df = pd.DataFrame(ground_truth_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(115885, 4)\n",
      "(115885, 5)\n"
     ]
    }
   ],
   "source": [
    "print(ground_truth_df.shape)\n",
    "print(extracted_features_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(115885, 7)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge features with ground truth\n",
    "feature_df = pd.merge(extracted_features_df, ground_truth_df, on=['video_idx', 'frame_idx'], how='left')\n",
    "feature_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_points = {\n",
    "#     0: 19716,  # Video 0\n",
    "#     1: 19719,  # Video 1\n",
    "#     2: 19432, # Video 2 \n",
    "# }\n",
    "\n",
    "# Assuming feature_df is the dataframe containing video_idx and frame_idx columns\n",
    "feature_df, split_overview = partition_feature_df(feature_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_idx</th>\n",
       "      <th>frame_idx</th>\n",
       "      <th>loudness_rms</th>\n",
       "      <th>zcr</th>\n",
       "      <th>rhythm</th>\n",
       "      <th>Kermit</th>\n",
       "      <th>Audio_StatlerWaldorf</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0-A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0-A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0-A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0-A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0-A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   video_idx  frame_idx  loudness_rms  zcr  rhythm  Kermit  \\\n",
       "0          0          0           0.0  0.0     0.0       0   \n",
       "1          0          1           0.0  0.0     0.0       0   \n",
       "2          0          2           0.0  0.0     0.0       0   \n",
       "3          0          3           0.0  0.0     0.0       0   \n",
       "4          0          4           0.0  0.0     0.0       0   \n",
       "\n",
       "   Audio_StatlerWaldorf fold  \n",
       "0                     0  0-A  \n",
       "1                     0  0-A  \n",
       "2                     0  0-A  \n",
       "3                     0  0-A  \n",
       "4                     0  0-A  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   video_idx fold  Kermit  Audio_StatlerWaldorf\n",
      "0          0  0-A    2916                  1015\n",
      "1          0  0-B    2533                   399\n",
      "2          1  1-A    3925                   542\n",
      "3          1  1-B    8327                   282\n",
      "4          2  2-A    5231                   171\n",
      "5          2  2-B   10277                   308\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['0-A', '0-B', '1-A', '1-B', '2-A', '2-B'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the results\n",
    "print(split_overview)\n",
    "\n",
    "feature_df['fold'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN in features: loudness_rms    0\n",
      "rhythm          0\n",
      "zcr             0\n",
      "dtype: int64\n",
      "\n",
      "Infinite values in features: loudness_rms    0\n",
      "rhythm          0\n",
      "zcr             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for invalid values\n",
    "print(\"NaN in features:\", feature_df[['loudness_rms', 'rhythm', 'zcr']].isnull().sum())\n",
    "print()\n",
    "print(\"Infinite values in features:\", np.isinf(feature_df[['loudness_rms', 'rhythm', 'zcr']]).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add the target variable to feature_df\n",
    "# feature_df['target'] = create_target_variable(feature_df)\n",
    "\n",
    "# print(\"Unique target values:\", feature_df['target'].unique())\n",
    "# Note Statler/Wald and Kermit have no simultanous 1s thus [0,1,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nested CV - KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kermit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outer Fold: 0-A\n",
      "  Inner Validation Fold: 0-B\n",
      "  Inner Validation Fold: 1-A\n",
      "  Inner Validation Fold: 1-B\n",
      "  Inner Validation Fold: 2-A\n",
      "  Inner Validation Fold: 2-B\n",
      "Metrics for Fold 0-A: {'outer_fold': '0-A', 'accuracy': 0.6859403530127814, 'precision': np.float64(0.7534524776829667), 'recall': np.float64(0.6859403530127814), 'f1': np.float64(0.7152562584077999), 'roc_auc': np.float64(0.5273406472499836)}\n",
      "Outer Fold: 0-B\n",
      "  Inner Validation Fold: 0-A\n",
      "  Inner Validation Fold: 1-A\n",
      "  Inner Validation Fold: 1-B\n",
      "  Inner Validation Fold: 2-A\n",
      "  Inner Validation Fold: 2-B\n",
      "Metrics for Fold 0-B: {'outer_fold': '0-B', 'accuracy': 0.686527814394938, 'precision': np.float64(0.7720901048139849), 'recall': np.float64(0.686527814394938), 'f1': np.float64(0.7230335517999853), 'roc_auc': np.float64(0.5170890064200269)}\n",
      "Outer Fold: 1-A\n",
      "  Inner Validation Fold: 0-A\n",
      "  Inner Validation Fold: 0-B\n",
      "  Inner Validation Fold: 1-B\n",
      "  Inner Validation Fold: 2-A\n",
      "  Inner Validation Fold: 2-B\n",
      "Metrics for Fold 1-A: {'outer_fold': '1-A', 'accuracy': 0.6613925655459202, 'precision': np.float64(0.685227624247085), 'recall': np.float64(0.6613925655459202), 'f1': np.float64(0.6725179022351464), 'roc_auc': np.float64(0.5206006634785926)}\n",
      "Outer Fold: 1-B\n",
      "  Inner Validation Fold: 0-A\n",
      "  Inner Validation Fold: 0-B\n",
      "  Inner Validation Fold: 1-A\n",
      "  Inner Validation Fold: 2-A\n",
      "  Inner Validation Fold: 2-B\n",
      "Metrics for Fold 1-B: {'outer_fold': '1-B', 'accuracy': 0.5483752040870069, 'precision': np.float64(0.5227179726287121), 'recall': np.float64(0.5483752040870069), 'f1': np.float64(0.4992384086814779), 'roc_auc': np.float64(0.5175382765573506)}\n",
      "Outer Fold: 2-A\n",
      "  Inner Validation Fold: 0-A\n",
      "  Inner Validation Fold: 0-B\n",
      "  Inner Validation Fold: 1-A\n",
      "  Inner Validation Fold: 1-B\n",
      "  Inner Validation Fold: 2-B\n",
      "Metrics for Fold 2-A: {'outer_fold': '2-A', 'accuracy': 0.6287566899958831, 'precision': np.float64(0.6118884014102822), 'recall': np.float64(0.6287566899958831), 'f1': np.float64(0.6195987190797513), 'roc_auc': np.float64(0.5108654885505074)}\n",
      "Outer Fold: 2-B\n",
      "  Inner Validation Fold: 0-A\n",
      "  Inner Validation Fold: 0-B\n",
      "  Inner Validation Fold: 1-A\n",
      "  Inner Validation Fold: 1-B\n",
      "  Inner Validation Fold: 2-A\n",
      "Metrics for Fold 2-B: {'outer_fold': '2-B', 'accuracy': 0.4645966642190286, 'precision': np.float64(0.4853954516543118), 'recall': np.float64(0.4645966642190286), 'f1': np.float64(0.3994323532141073), 'roc_auc': np.float64(0.48721491043526116)}\n",
      "\n",
      "Overall Summary:\n",
      "{'accuracy': 0.6125982152092597, 'precision': 0.6384620054062239, 'recall': 0.6125982152092597, 'f1': 0.6048461989030447, 'roc_auc': 0.5134414987819538}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "\n",
    "# Define KNN model\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan'] \n",
    "}\n",
    "\n",
    "train_cols = ['loudness_rms', 'rhythm', 'zcr']\n",
    "target_col = 'Kermit'\n",
    "results_kermit, summary_kermit, best_model_kermit = nested_cross_validation(feature_df, train_cols, target_col, KNeighborsClassifier, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0-A': KNeighborsClassifier(metric='euclidean', n_neighbors=3, weights='distance'),\n",
       " '0-B': KNeighborsClassifier(metric='euclidean', n_neighbors=3, weights='distance'),\n",
       " '1-A': KNeighborsClassifier(metric='euclidean', n_neighbors=3, weights='distance'),\n",
       " '1-B': KNeighborsClassifier(metric='euclidean', n_neighbors=3, weights='distance'),\n",
       " '2-A': KNeighborsClassifier(metric='euclidean', n_neighbors=3, weights='distance'),\n",
       " '2-B': KNeighborsClassifier(metric='euclidean', n_neighbors=3, weights='distance')}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  TODO:\n",
    "#     - schreib so um dass direkt ein cleaner Test-Train Split\n",
    "#     - Einmal Kermit und einmal die Alten   \n",
    "#     - add features aus sim2\n",
    "#     - mach jeweils KNN (Hedger) vs RF (seperator)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define KNN model\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9],  # Ensure these are smaller than the smallest fold size\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan']  # Supported metrics\n",
    "}\n",
    "\n",
    "train_cols = ['loudness_rms', 'rhythm', 'zcr']\n",
    "target_col = 'Audio_StatlerWaldorf'\n",
    "results_statler, summary_statler, best_models_statler = nested_cross_validation(feature_df, train_cols, target_col, KNeighborsClassifier, param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sim1_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
