{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Feature Analyses SIM 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from scripts.load_data import check_and_load\n",
    "from scripts.extract_audio_features import extract_zcr, extract_loudness, extract_rhythm, create_target_variable\n",
    "from scipy.signal import correlate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define paths\n",
    "# data_path = \"../ground_truth_data/trimmed_videos\"\n",
    "# frames_output_dir = \"../ground_truth_data/trimmed_videos/frames\"\n",
    "# audio_output_dir = \"../ground_truth_data/trimmed_videos/audio\"\n",
    "# annotations_path = \"../ground_truth_data/trimmed_videos\"\n",
    "\n",
    "data_path = \"../ground_truth_data\"\n",
    "frames_output_dir = \"../ground_truth_data/frames\"\n",
    "audio_output_dir = \"../ground_truth_data/audio\"\n",
    "annotations_path = \"../ground_truth_data\"\n",
    "\n",
    "muppet_files = {\n",
    "    \"Muppets-02-01-01.avi\": \"GroundTruth_Muppets-02-01-01.csv\",\n",
    "    \"Muppets-02-04-04.avi\": \"GroundTruth_Muppets-02-04-04.csv\",\n",
    "    \"Muppets-03-04-03.avi\": \"GroundTruth_Muppets-03-04-03.csv\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames and audio are already extracted.\n",
      "Loading audio segments...\n",
      "Loaded 3 audio files.\n",
      "Loaded audio segments for 3 videos.\n",
      "Loaded frames for 3 videos.\n",
      "Number of videos with frames: 3\n",
      "Video 0 has 38681 frames.\n",
      "Video 1 has 38706 frames.\n",
      "Video 2 has 38498 frames.\n"
     ]
    }
   ],
   "source": [
    "# TODO: add code to overwrite frames or audio in case only one exists\n",
    "annotations, audio_data, frames = check_and_load(data_path, frames_output_dir, audio_output_dir, annotations_path, muppet_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loudness\n",
    "\n",
    "    - Description: Measures overall signal strength through RMS of sample amplitudes.\n",
    "    - Use Case: Effective when differentiating characters based on the volume or energy of their speech or sound.\n",
    "\n",
    "Fundamental Frequency\n",
    "\n",
    "    - Description: Extracts the pitch of the audio signal using methods like zero-crossing rate (ZCR).\n",
    "    - Use Case: Useful for identifying characters with distinct pitch or tonal qualities in their voices (e.g., Kermit's high-pitched voice).\n",
    "\n",
    "Rhythm Detection\n",
    "\n",
    "    - Description: Uses autocorrelation to find repeating patterns across frames. Statistical moments indicate the presence of rhythm.\n",
    "    - Use Case: Ideal for characters with unique speech cadences or rhythmic patterns (e.g., the conversational style of Waldorf and Statler)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ZCR features\n",
    "zcr_features = extract_zcr(audio_data)\n",
    "\n",
    "# Extract loudness features\n",
    "loudness_features = extract_loudness(audio_data)\n",
    "\n",
    "# Extract rhythm features\n",
    "rhythm_features = extract_rhythm(audio_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(115888, 5)\n"
     ]
    }
   ],
   "source": [
    "extracted_features_df = []\n",
    "\n",
    "for video_idx, (audio_entry, zcr, loudness, rhythm) in enumerate(zip(audio_data, zcr_features, loudness_features, rhythm_features)):\n",
    "    if zcr is not None and loudness is not None and rhythm is not None:\n",
    "        num_frames = min(len(zcr), len(loudness), len(rhythm))\n",
    "        for frame_idx in range(num_frames):\n",
    "            extracted_features_df.append({\n",
    "                \"video_idx\": video_idx,\n",
    "                \"frame_idx\": frame_idx,\n",
    "                \"loudness_rms\": loudness[frame_idx],\n",
    "                \"zcr\": zcr[frame_idx],\n",
    "                \"rhythm\": rhythm[frame_idx]\n",
    "            })\n",
    "\n",
    "extracted_features_df = pd.DataFrame(extracted_features_df)\n",
    "print(extracted_features_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video 0: (38682, 5)\n",
      "Video 1: (38707, 5)\n",
      "Video 2: (38499, 5)\n"
     ]
    }
   ],
   "source": [
    "for i in extracted_features_df[\"video_idx\"].unique():\n",
    "    print(f\"Video {i}: {extracted_features_df[extracted_features_df['video_idx'] == i].shape}\")\n",
    "\n",
    "\n",
    "# Video 0 has 38681 frames.\n",
    "# Video 1 has 38706 frames.\n",
    "# Video 2 has 38498 frames.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from filenames to video indices\n",
    "video_idx_map = {filename: idx for idx, filename in enumerate(muppet_files.keys())}\n",
    "\n",
    "# Prepare ground truth data with corrected video_idx\n",
    "ground_truth_data = []\n",
    "for video_filename, annotation_df in annotations.items():\n",
    "    video_idx = video_idx_map[video_filename]  # Map video filename to its index\n",
    "    for _, row in annotation_df.iterrows():\n",
    "        ground_truth_data.append({\n",
    "            'video_idx': video_idx,  # Use mapped video index\n",
    "            'frame_idx': row['Frame_number'],  # Assuming Frame_number exists\n",
    "            'Kermit': row['Kermit'],  # Assuming Kermit is a column in the annotation\n",
    "            'Audio_StatlerWaldorf': row['Audio_StatlerWaldorf']  # Assuming this column exists\n",
    "        })\n",
    "\n",
    "# Create a DataFrame for ground truth\n",
    "ground_truth_df = pd.DataFrame(ground_truth_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(115885, 4)\n",
      "(115852, 5)\n"
     ]
    }
   ],
   "source": [
    "print(ground_truth_df.shape)\n",
    "print(extracted_features_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(115852, 7)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge features with ground truth\n",
    "feature_df = pd.merge(extracted_features_df, ground_truth_df, on=['video_idx', 'frame_idx'], how='left')\n",
    "feature_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the target variable to feature_df\n",
    "feature_df['target'] = create_target_variable(feature_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Data (Replace with your feature_df data)\n",
    "X = feature_df[['loudness_rms', 'rhythm', 'zcr']].values\n",
    "y = feature_df['target'].values  # Replace 'Kermit' with your target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of         video_idx  frame_idx  loudness_rms  zcr  rhythm  Kermit  \\\n",
       "0               0          0           0.0  0.0     0.0       0   \n",
       "1               0          1           0.0  0.0     0.0       0   \n",
       "2               0          2           0.0  0.0     0.0       0   \n",
       "3               0          3           0.0  0.0     0.0       0   \n",
       "4               0          4           0.0  0.0     0.0       0   \n",
       "...           ...        ...           ...  ...     ...     ...   \n",
       "115847          2      38481           0.0  0.0     0.0       0   \n",
       "115848          2      38482           0.0  0.0     0.0       0   \n",
       "115849          2      38483           0.0  0.0     0.0       0   \n",
       "115850          2      38484           0.0  0.0     0.0       0   \n",
       "115851          2      38485           0.0  0.0     0.0       0   \n",
       "\n",
       "        Audio_StatlerWaldorf  target  \n",
       "0                          0       0  \n",
       "1                          0       0  \n",
       "2                          0       0  \n",
       "3                          0       0  \n",
       "4                          0       0  \n",
       "...                      ...     ...  \n",
       "115847                     0       0  \n",
       "115848                     0       0  \n",
       "115849                     0       0  \n",
       "115850                     0       0  \n",
       "115851                     0       0  \n",
       "\n",
       "[115852 rows x 8 columns]>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1120)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique target values: [0 1 2]\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique target values:\", feature_df['target'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video_idx               0\n",
      "frame_idx               0\n",
      "loudness_rms            0\n",
      "zcr                     0\n",
      "rhythm                  0\n",
      "Kermit                  0\n",
      "Audio_StatlerWaldorf    0\n",
      "target                  0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(feature_df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN in features: loudness_rms    0\n",
      "rhythm          0\n",
      "zcr             0\n",
      "dtype: int64\n",
      "Infinite values in features: loudness_rms    0\n",
      "rhythm          0\n",
      "zcr             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for invalid values\n",
    "print(\"NaN in features:\", feature_df[['loudness_rms', 'rhythm', 'zcr']].isnull().sum())\n",
    "print(\"Infinite values in features:\", np.isinf(feature_df[['loudness_rms', 'rhythm', 'zcr']]).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.metrics import (\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    average_precision_score,\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    classification_report\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define KNN model\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9],  # Ensure these are smaller than the smallest fold size\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan']  # Supported metrics\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "Best Parameters: {'metric': 'euclidean', 'n_neighbors': 9, 'weights': 'uniform'}\n",
      "Best F1 Score: 0.6598352300117785\n"
     ]
    }
   ],
   "source": [
    "# Use StratifiedKFold for balanced splits\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1120)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=knn,\n",
    "    param_grid=param_grid,\n",
    "    scoring='recall_weighted',  # Adjust scoring metric as needed\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Perform grid search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and the corresponding score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best F1 Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.90      0.78     23868\n",
      "           1       0.33      0.11      0.17     10049\n",
      "           2       0.00      0.00      0.00       839\n",
      "\n",
      "    accuracy                           0.65     34756\n",
      "   macro avg       0.34      0.34      0.32     34756\n",
      "weighted avg       0.57      0.65      0.59     34756\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on the test set\n",
    "best_knn = grid_search.best_estimator_\n",
    "y_pred = best_knn.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiclass AUC Score: 0.5321061417251541\n"
     ]
    }
   ],
   "source": [
    "# Get the best estimator from the grid search\n",
    "best_knn = grid_search.best_estimator_\n",
    "# Predict probabilities for all classes\n",
    "y_proba = best_knn.predict_proba(X_test)\n",
    "\n",
    "# Calculate the AUC for multiclass\n",
    "auc = roc_auc_score(y_test, y_proba, multi_class='ovr')  # 'ovr' or 'ovo'\n",
    "\n",
    "# Print the AUC\n",
    "print(\"Multiclass AUC Score:\", auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert labels for the first character\n",
    "y_test_kermit = (y_test == 1) | (y_test == 3)\n",
    "y_pred_kermit = (y_pred == 1) | (y_pred == 3)\n",
    "\n",
    "# Convert labels for the second character\n",
    "y_test_wald_stat = (y_test == 2) | (y_test == 3)\n",
    "y_pred_wald_stat = (y_pred == 2) | (y_pred == 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6756243526297617\n"
     ]
    }
   ],
   "source": [
    "# Compute metrics for the first character\n",
    "accuracy_kermit = accuracy_score(y_test_kermit, y_pred_kermit)\n",
    "precision_kermit = precision_score(y_test_kermit, y_pred_kermit)\n",
    "recall_kermit = recall_score(y_test_kermit, y_pred_kermit)\n",
    "f1_kermit = f1_score(y_test_kermit, y_pred_kermit)\n",
    "map_kermit = average_precision_score(y_test_kermit, y_pred_kermit)\n",
    "\n",
    "print(accuracy_kermit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wald accuracy: 0.9758027390954079\n",
      "wald accuracy: 0.0\n",
      "wald accuracy: 0.0\n",
      "wald accuracy: 0.0\n",
      "wald accuracy: 0.024139716883415815\n"
     ]
    }
   ],
   "source": [
    "# Compute metrics for the second character\n",
    "accuracy_wald_stat = accuracy_score(y_test_wald_stat, y_pred_wald_stat)\n",
    "print(f\"wald accuracy: {accuracy_wald_stat}\")\n",
    "\n",
    "precision_wald_stat = precision_score(y_test_wald_stat, y_pred_wald_stat)\n",
    "print(f\"wald accuracy: {precision_wald_stat}\")\n",
    "\n",
    "recall_wald_stat = recall_score(y_test_wald_stat, y_pred_wald_stat)\n",
    "print(f\"wald accuracy: {recall_wald_stat}\")\n",
    "\n",
    "f1_wald_stat = f1_score(y_test_wald_stat, y_pred_wald_stat)\n",
    "print(f\"wald accuracy: {f1_wald_stat}\")\n",
    "\n",
    "map_wald_stat = average_precision_score(y_test_wald_stat, y_pred_wald_stat)\n",
    "print(f\"wald accuracy: {map_wald_stat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "general accuracy: 0.6535849925192773\n",
      "general precision: 0.5687129153956213\n",
      "general recall: 0.6535849925192773\n",
      "general f1: 0.58670871905626\n"
     ]
    }
   ],
   "source": [
    "# Compute metrics for the general classifier as a whole\n",
    "accuracy_general = accuracy_score(y_test, y_pred)\n",
    "print(f\"general accuracy: {accuracy_general}\")\n",
    "\n",
    "precision_general = precision_score(y_test, y_pred, average=\"weighted\")\n",
    "print(f\"general precision: {precision_general}\")\n",
    "\n",
    "recall_general = recall_score(y_test, y_pred, average=\"weighted\")\n",
    "print(f\"general recall: {recall_general}\")\n",
    "\n",
    "f1_general = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "print(f\"general f1: {f1_general}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sm_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
