{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Modeling 2 Summary\n",
    "**Parts of the notebook are the same as in Similarity Modeling 1 (general approach)**. \n",
    "\n",
    "**Authors: Sakka Mahmoud Abdussalem, Kravchenko Oleksandra**\n",
    "\n",
    "This notebook contains a summary about our work distribution, task approach and results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timesheets\n",
    "**Oleksandra's timesheet**\n",
    "<table>\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th>Date</th>\n",
    "    <th>Task</th>\n",
    "    <th>Hours</th>\n",
    "\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td>24.10.2024</td>\n",
    "    <td>Initial Team Meeting</td>\n",
    "    <td>0.5</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>01.12.2024</td>\n",
    "    <td>Second Team Meeting - Discussion of extraction approach and work split</td>\n",
    "    <td>0.5</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>08.12.2024</td>\n",
    "    <td>Third Team Meeting</td>\n",
    "    <td>0.5</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "**Mahmoud's timesheet:**\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th>Date</th>\n",
    "    <th>Task</th>\n",
    "    <th>Hours</th>\n",
    "\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td>24.10.2024</td>\n",
    "    <td>Initial Team Meeting</td>\n",
    "    <td>0.5</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>01.12.2024</td>\n",
    "    <td>Second Team Meeting - Discussion of extraction approach and work split</td>\n",
    "    <td>0.5</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>06.12.2024</td>\n",
    "    <td>Initial exploration of material, implementation of trim_video.py and audio_extraction</td>\n",
    "    <td>4</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>07.12.2024</td>\n",
    "    <td>Implementation of load_data.py</td>\n",
    "    <td>2</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>08.12.2024</td>\n",
    "    <td>Third Team Meeting</td>\n",
    "    <td>0.5</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work distribution\n",
    "Since we both participated in both Similarity Modeling 1 and 2 courses, we split the work in a way that allows us to experiment in all aspects of video analysis and classification. For Similarity Modeling 2, Mahmoud's focus was on the video features and Oleksandra's on the audio features (the roles are reversed in the Similarity Modeling 1). Therefore, we both also contributed to the hybrid approach by creating separate features and merging them together. The analytical work was also shared and conducted in regular discussion and iterative improvements of all notebooks and functions. The timesheets give a more clear indication of the work distribution within the group. Helper functions for frame extraction, data loading, cross validation, evaluation and so on were written in collaboration and refined for the specific needs that came up during the project. The summaries, analyses were mainly written by the authors of the respective notebooks, same goes for indivudal sections in this notebook. \n",
    "\n",
    "## Approach overview\n",
    "We focused on experimentation with multiple features mentioned in the Similarity Modeling 2 lectures and their variants. We brainstormed possible features and classification algorithms to cover a broad spectrum of the domain. During the feature engineering, we inspected different features on excerpts of the data, tunes them, used them in the classification models, and refined the methods based on the evaluation results. We used a GitHub repository as a collaborative environment for the project. \n",
    "Regarding overall technical setup, we created a script `load_data.py` that extract frames and audio tracks from the videos and organises them in the folders on our local machines. The feature extraction functions then reference those files to create datasets, that are then also saved and reused to save time. Our classification model were either trained and evaluated using nested cross validation method or holdout set depending on the computational depends of the task. \n",
    "Our train-test-split approach accounts for the time-series nature of the data. We manually define split points that correspon to \"fade-outs\" in the data to correctly split the videos and ensure that the same scene is not present in both train and test split to avoid data leakage and overfitting. \n",
    "The evaluation was done using binary accuracy, precision, recall and f1 metrics, as well as ROC curve. Since our data is highly imabalanced (target characters are much more often are absent than present), we found this evaluation method to be most accurate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification using audio features\n",
    "### Features\n",
    "### Results\n",
    "### Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification using video features\n",
    "### Features\n",
    "### Results\n",
    "### Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid classification\n",
    "### Features\n",
    "### Results\n",
    "### Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
